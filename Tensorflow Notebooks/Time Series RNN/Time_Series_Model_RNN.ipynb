{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZBFzjClURz"
      },
      "source": [
        "# Build and train a RNN model for time series data\n",
        "\n",
        "Author: Robin Baumann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf-dClTNZaIo"
      },
      "source": [
        "## Import and Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PBJBv1jEtJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Load TensorFlow and shwo versions\n",
        "!pip install tensorflow==2.5\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load Tensorflow optimizations\n",
        "!pip install tensorflow_model_optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Load TensorFlow Lites C converter\n",
        "!pip install\n",
        "from tensorflow.lite.python.util import convert_bytes_to_c_source\n",
        "\n",
        "# Numpy \n",
        "!pip install numpy\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib \n",
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Python's math library\n",
        "!pip install math\n",
        "import math\n",
        "\n",
        "# Define paths to model files\n",
        "!pip install os\n",
        "import os\n",
        "!pip install shutil\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print tensorflow versions\n",
        "print('TF Version', tf.__version__)\n",
        "print('Keras Version', tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear backend and set random seeds for reproducability\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UQblnrLd_ET"
      },
      "source": [
        "## Configure directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PYwRFppd-WB",
        "outputId": "c0525026-b186-4041-8a3d-f52ea36ee9d0"
      },
      "outputs": [],
      "source": [
        "# Folder for all Models\n",
        "MODELS_DIR = 'models/'\n",
        "if os.path.exists(MODELS_DIR):\n",
        "    shutil.rmtree(MODELS_DIR)\n",
        "os.mkdir(MODELS_DIR)\n",
        "\n",
        "# Folder for all Data\n",
        "DATA_DIR = 'data/'\n",
        "if os.path.exists(DATA_DIR):\n",
        "    shutil.rmtree(DATA_DIR)\n",
        "os.mkdir(DATA_DIR)\n",
        "\n",
        "# Tensorboard logs\n",
        "LOG_DIR = 'logs/'\n",
        "if os.path.exists(LOG_DIR):\n",
        "    shutil.rmtree(LOG_DIR)\n",
        "os.mkdir(LOG_DIR)\n",
        "    \n",
        "# Directorys for Models\n",
        "MODEL_NAME = 'time_series_model_rnn'\n",
        "MODEL_TF = MODELS_DIR + MODEL_NAME\n",
        "MODEL_KERAS = MODEL_TF + '.h5'\n",
        "MODEL_TF_QUANT = MODELS_DIR + MODEL_NAME + '_quant'\n",
        "MODEL_KERAS_QUANT = MODEL_TF_QUANT + '.h5'\n",
        "MODEL_TFLITE = MODELS_DIR + MODEL_NAME + '.tflite'\n",
        "MODEL_TFLITE_HEADER = MODELS_DIR + MODEL_NAME + '.h'\n",
        "MODEL_TFLITE_QUANT = MODELS_DIR + MODEL_NAME + '_quant.tflite'\n",
        "MODEL_TFLITE_HEADER_QUANT = MODELS_DIR + MODEL_NAME + '_quant.h'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PuBEb6CMeo"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gB0-dlNmLT-"
      },
      "source": [
        "### 1. Generate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cfEdYAeud1_"
      },
      "source": [
        "#### Sunspot Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUkBe7t2uijJ"
      },
      "outputs": [],
      "source": [
        "# # import wget to load Data\n",
        "# !pip install wget\n",
        "# import wget\n",
        "\n",
        "# # Get Test Dataset\n",
        "# fielname = wget.download('https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv', out=DATA_DIR)\n",
        "\n",
        "# # import Dependencie to handel csv files\n",
        "# import csv\n",
        "\n",
        "# # Read Data from zip\n",
        "# time_step = []\n",
        "# sunspots = []\n",
        "# with open(fielname) as csvfile:\n",
        "#   reader = csv.reader(csvfile, delimiter=',')\n",
        "#   next(reader)\n",
        "#   for row in reader:\n",
        "#     sunspots.append(float(row[2]))\n",
        "#     time_step.append(int(row[0]))\n",
        "\n",
        "# # Define Data\n",
        "# time = np.array(time_step)\n",
        "# true_values = np.array(sunspots)\n",
        "\n",
        "# # define Inputs\n",
        "# input_1 = true_values.copy()\n",
        "# for t in range(len(input_1)):\n",
        "#   input_1[t] += 0.1 * input_1[t - 10]\n",
        "#   input_1[t] += 0.8 * input_1[t - 2]\n",
        "# input_1 = input_1*42\n",
        "\n",
        "# input_2 = true_values.copy()\n",
        "# input_2 +=  max(input_2)*np.cos(time*np.pi)\n",
        "# input_2 = input_2+42.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-fQ476vu2wI"
      },
      "source": [
        "#### Sinus Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKjg7QeMDsDx"
      },
      "outputs": [],
      "source": [
        "# Create time vector for a Period of 10 pi\n",
        "time = np.linspace(0, 10*np.pi, num=1000)\n",
        "\n",
        "# Create sinus for time interval (with noise)\n",
        "true_values = np.sin(time)\n",
        "\n",
        "# Create cos as inputs for network (with noise)\n",
        "input_1 = np.sin(time)+2*np.cos(time)**2\n",
        "#input_1 += 0.05 * np.random.randn(*input_1.shape)\n",
        "input_2 = np.sin(time)**2*np.cos(time)\n",
        "#input_2 += 0.05 * np.random.randn(*input_2.shape)\n",
        "\n",
        "# Plot the data\n",
        "# True data\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('True Values (logits)')\n",
        "plt.plot(time, true_values, 'k-', label=\"true_values\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# input_1\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('Input 1')\n",
        "plt.plot(time, input_1, 'k-', label=\"input_1\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# input_2\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('Input 2')\n",
        "plt.plot(time, input_2, 'k-', label=\"input_2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5deIJ1B5ZidN"
      },
      "source": [
        "### 2. Normalize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTfeAb2EZhgt"
      },
      "outputs": [],
      "source": [
        "# Get Range of data and Normalize the data\n",
        "VALUES_RANGE = round(max(true_values)-min(true_values), 3)\n",
        "VALUES_MEAN = round(np.mean(true_values), 3)\n",
        "true_values = (true_values-VALUES_MEAN)/VALUES_RANGE\n",
        "\n",
        "INPUT_1_RANGE = round(max(input_1)-min(input_1), 3)\n",
        "INPUT_1_MEAN = round(np.mean(input_1), 3)\n",
        "input_1 = (input_1-INPUT_1_MEAN)/INPUT_1_RANGE\n",
        "\n",
        "INPUT_2_RANGE = round(max(input_2)-min(input_2), 3)\n",
        "INPUT_2_MEAN = round(np.mean(input_2), 3)\n",
        "input_2 = (input_2-INPUT_2_MEAN)/INPUT_2_RANGE\n",
        "\n",
        "# calculate step size\n",
        "STEP_SIZE = round(time[1]-time[0], 5)\n",
        "\n",
        "# Print Values\n",
        "print('VALUES_RANGE: ', VALUES_RANGE)\n",
        "print('VALUES_MEAN: ', VALUES_MEAN)\n",
        "print('INPUT_1_RANGE: ', INPUT_1_RANGE)\n",
        "print('INPUT_1_MEAN: ', INPUT_1_MEAN)\n",
        "print('INPUT_2_RANGE: ', INPUT_2_RANGE)\n",
        "print('INPUT_2_MEAN: ', INPUT_2_MEAN)\n",
        "print('STEP_SIZE: ', STEP_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8Xk_pMH4Rt"
      },
      "source": [
        "### 3. Split the Data and Create Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "nNYko5L1keqZ",
        "outputId": "c438949b-001a-4c21-cd85-795bc33e1d1f"
      },
      "outputs": [],
      "source": [
        "# 60% of our data for training and 20% for testing, remaining 20% for validation\n",
        "train_split =  int(0.6 * len(time))\n",
        "test_split = int(0.2*len(time) + train_split)\n",
        "\n",
        "# Use np.split to chop data into three parts.\n",
        "# True data\n",
        "time_train, time_validate, time_test = np.split(time, [train_split,test_split])\n",
        "true_values_train, true_values_validate, true_values_test = np.split(true_values, [train_split,test_split])\n",
        "# input_1\n",
        "input_1_train, input_1_validate, input_1_test = np.split(input_1, [train_split,test_split])\n",
        "# input_2\n",
        "input_2_train, input_2_validate, input_2_test = np.split(input_2, [train_split,test_split])\n",
        "\n",
        "# Plot the data in each partition in different colors:\n",
        "# True data\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('True Values (logits)')\n",
        "plt.plot(time_train, true_values_train, 'b-', label=\"Train\")\n",
        "plt.plot(time_test, true_values_test, 'r-', label=\"Test\")\n",
        "plt.plot(time_validate, true_values_validate, 'g-', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# input_1\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('Input 1')\n",
        "plt.plot(time_train, input_1_train, 'b-', label=\"Train\")\n",
        "plt.plot(time_test, input_1_test, 'r-', label=\"Test\")\n",
        "plt.plot(time_validate, input_1_validate, 'g-', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# input_2\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title('Input 2')\n",
        "plt.plot(time_train, input_2_train, 'b-', label=\"Train\")\n",
        "plt.plot(time_test, input_2_test, 'r-', label=\"Test\")\n",
        "plt.plot(time_validate, input_2_validate, 'g-', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfdelu1TmgPk"
      },
      "source": [
        "## Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfpXZS_M0Wbl"
      },
      "source": [
        "### 1. Define the Model Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5sHjRcI0wtr"
      },
      "outputs": [],
      "source": [
        "# Model and Training Hyperparameters\n",
        "UNITS = 32\n",
        "ACTIVATION = 'relu'\n",
        "EPOCHS = 1000\n",
        "DROPOUT = 0.1\n",
        "WINDOW_SIZE = 16\n",
        "BATCH_SIZE_TRAIN = 64\n",
        "BATCH_SIZE_INFERENCE = 1\n",
        "NUM_FEATURES = 2\n",
        "NUM_OUTPUTS = 1\n",
        "LEARNING_RATE = 1e-6\n",
        "LOSS = 'mean_squared_error'\n",
        "METRIC = 'mae'\n",
        "OPTIMIZER = tf.optimizers.Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57jSGk5ZrGEs"
      },
      "source": [
        "### 2. Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Model(batch_size, quantize_aware=False, weights=False):\n",
        "\n",
        "    sequential_model = tf.keras.models.Sequential([\n",
        "        \n",
        "        # Input Layer \n",
        "        tf.keras.layers.InputLayer(input_shape=(WINDOW_SIZE, NUM_FEATURES), batch_size=batch_size),\n",
        "\n",
        "        # RNN layer with quantization\n",
        "        tf.keras.layers.LSTM(UNITS, activation=ACTIVATION, dropout=DROPOUT, return_sequences=True),\n",
        "        tf.keras.layers.LSTM(UNITS, activation=ACTIVATION, dropout=DROPOUT),\n",
        "\n",
        "        # Add Single Output Layer (perceptron)\n",
        "        tf.keras.layers.Dense(NUM_OUTPUTS)])\n",
        "        \n",
        "    return sequential_model\n",
        "\n",
        "# Function to compile Model\n",
        "def Compile_Model(model):\n",
        "    model.compile(loss=LOSS, optimizer=OPTIMIZER(learning_rate=LEARNING_RATE), metrics=[METRIC])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Ewq-8CLHz2"
      },
      "source": [
        "## Build Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Create Function to build Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiZLDqvfLIQb"
      },
      "outputs": [],
      "source": [
        "# Create widowed Dataset\n",
        "def get_data(true_values, input_1, input_2, batch_size):\n",
        "\n",
        "  # the length of data series (rounded with respect to batch size for Training)\n",
        "  n_datapoints = (len(true_values)-WINDOW_SIZE)//batch_size*batch_size\n",
        "\n",
        "  # set array with indices to loop through\n",
        "  idx = np.arange(n_datapoints)\n",
        "\n",
        "  # Get input and output Batch \n",
        "  input_1 = [input_1[i : i+WINDOW_SIZE] for i in idx]\n",
        "  input_2 = [input_2[i : i+WINDOW_SIZE] for i in idx]\n",
        "  output_data = [true_values[i+WINDOW_SIZE] for i in idx]\n",
        "\n",
        "  # Interweaving of input data\n",
        "  input_data = []\n",
        "  for ii in range(n_datapoints):\n",
        "    for jj in range(WINDOW_SIZE):\n",
        "      input_data.append(input_1[ii][jj])\n",
        "      input_data.append(input_2[ii][jj])\n",
        "\n",
        "  # Reshape Data to wanted format\n",
        "  input_data = np.reshape(input_data, [n_datapoints, WINDOW_SIZE, NUM_FEATURES])\n",
        "  output_data = np.reshape(output_data, [n_datapoints, NUM_OUTPUTS])\n",
        "      \n",
        "      \n",
        "  return input_data, output_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create Datasets for Training and Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset for training\n",
        "input_train, true_output_train = get_data(true_values_train, input_1_train, input_2_train, BATCH_SIZE_TRAIN)\n",
        "input_val, true_output_val = get_data(true_values_validate, input_1_validate, input_2_validate, BATCH_SIZE_TRAIN)\n",
        "input_test, true_output_test = get_data(true_values_test, input_1_test, input_2_test, BATCH_SIZE_TRAIN)\n",
        "\n",
        "# Dataset for Inference\n",
        "input_train_inf, true_output_train_inf = get_data(true_values_train, input_1_train, input_2_train, BATCH_SIZE_INFERENCE)\n",
        "input_val_inf, true_output_val_inf = get_data(true_values_validate, input_1_validate, input_2_validate, BATCH_SIZE_INFERENCE)\n",
        "input_test_inf, true_output_test_inf = get_data(true_values_test, input_1_test, input_2_test, BATCH_SIZE_INFERENCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5McVnHmNiDw"
      },
      "source": [
        "## Train Base Model and Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCk58WqJ9KVa"
      },
      "source": [
        "### 1. Define Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03TNOUa_9KVa"
      },
      "outputs": [],
      "source": [
        "# Callback for to stop early\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=50)\n",
        "\n",
        "# Callback to restore best Weight during training\n",
        "early_stop_and_restore_weights = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=100, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCNWPeu9KVb"
      },
      "source": [
        "### 2. Define Learining Rate Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-8xK9bA9KVb"
      },
      "outputs": [],
      "source": [
        "# init vector and order\n",
        "learning_rates = [LEARNING_RATE]\n",
        "order = -1\n",
        "\n",
        "# append Learing rates from LEARNING_RATE to 10^2\n",
        "while learning_rates[-1] < 10:\n",
        "    order += 1\n",
        "    for scale in range(2,12,2):\n",
        "        learning_rates.append(learning_rates[0] * 10**order * scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32QKu29U9KVb"
      },
      "source": [
        "### 4. Train with different Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD60bE8cXQId",
        "outputId": "d01fe9cf-86a5-4df3-b1b9-a2d0680748e4"
      },
      "outputs": [],
      "source": [
        "# # init vector with calculated Mean Absolut Errors\n",
        "# maes = []\n",
        "\n",
        "# # Train Model for 100 different learining rates\n",
        "# for learning_rate in learning_rates:\n",
        "\n",
        "#   # set new learning rate\n",
        "#   LEARNING_RATE = learning_rate\n",
        "\n",
        "#   # build Model with defined Model Class\n",
        "#   model_lr = Model(batch_size=BATCH_SIZE_TRAIN)\n",
        "\n",
        "#   # Compile the model\n",
        "#   Compile_Model(model_lr)\n",
        "\n",
        "#   # Run training step through network\n",
        "#   history = model_lr.fit( input_train, true_output_train,\n",
        "#                           validation_data=(input_val, true_output_val),\n",
        "#                           epochs=EPOCHS, \n",
        "#                           batch_size=BATCH_SIZE_TRAIN, \n",
        "#                           shuffle=True, \n",
        "#                           callbacks=[early_stopping],\n",
        "#                           verbose=0)\n",
        "\n",
        "#   # Get best mae on training for current learning rate\n",
        "#   mae = min(history.history['val_mae'])*VALUES_RANGE\n",
        "#   maes.append(mae)\n",
        "\n",
        "#   # print MAE for this LR\n",
        "#   print(f'learning rate: {learning_rate} -> MAE: {round(mae,3)} ~ {round(100*mae/VALUES_RANGE,3)}%')\n",
        "\n",
        "#   # Stop learning if mae for last 3 LRs was nan\n",
        "#   if math.isnan(mae):\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz-GbI7KuLu_"
      },
      "source": [
        "### 5. Plot MAE over Learning Rates and select best one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEApqY8Pt16u"
      },
      "outputs": [],
      "source": [
        "# # skip first and last Values to get better Plot\n",
        "# skip_begin = 0\n",
        "# skip_end = 3\n",
        "\n",
        "# # Plot Error over Learing Rates\n",
        "# plt.figure(figsize=(15,3))\n",
        "# plt.semilogx(learning_rates[skip_begin:len(maes)-skip_end], maes[skip_begin:-skip_end], 'k')\n",
        "# plt.title('Mean Absolut Error over Learningrate')\n",
        "# plt.xlabel('Learning Rate')\n",
        "# plt.ylabel('MAE')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # select best learning rate\n",
        "# lr_idx = [i for i, mae in enumerate(maes) if mae==min(maes)][0]\n",
        "# LEARNING_RATE = learning_rates[lr_idx]\n",
        "# print('BEST LEARNING RATE FOUND:', LEARNING_RATE)\n",
        "\n",
        "LEARNING_RATE = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpz4ZttMDhw"
      },
      "source": [
        "### 6. Learning with optimal Learining Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjbTn0DOMC9m"
      },
      "outputs": [],
      "source": [
        "# build Model with defined Model Class for Training\n",
        "model = Model(batch_size=BATCH_SIZE_TRAIN)\n",
        "\n",
        "# Compile the model\n",
        "Compile_Model(model)\n",
        "\n",
        "# Run training step through network\n",
        "history = model.fit( input_train, true_output_train,\n",
        "                        validation_data=(input_val, true_output_val),\n",
        "                        epochs=EPOCHS, \n",
        "                        batch_size=BATCH_SIZE_TRAIN, \n",
        "                        shuffle=True,\n",
        "                        callbacks=[early_stop_and_restore_weights],\n",
        "                        verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Fix Model Input Size for Inference and Show model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model again with batch size for inference\n",
        "model_inference = Model(batch_size=BATCH_SIZE_INFERENCE)\n",
        "\n",
        "# Set the trained weight for the new model\n",
        "model_inference.set_weights(model.get_weights())\n",
        "\n",
        "# Save model as Keras and tf model\n",
        "model_inference.save(MODEL_KERAS, save_format=\"h5\")\n",
        "model_inference.save(MODEL_TF, save_format=\"tf\")\n",
        "\n",
        "# show Model summary\n",
        "model_inference.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH32KNnh9KVd"
      },
      "source": [
        "### 8. Plot Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRE8KpEqVfaS"
      },
      "source": [
        "#### 8.1 Plot Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmvA-ksoln8r"
      },
      "outputs": [],
      "source": [
        "# Training Data\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# plot\n",
        "SKIP = 3\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.plot(np.arange(1, len(val_loss)+1, len(val_loss)/len(train_loss))[SKIP:], train_loss[SKIP:], 'k', label='Training loss')\n",
        "plt.plot(np.arange(1, len(val_loss)+1)[SKIP:], val_loss[SKIP:], 'k--', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 8.2 Predict output for all datasets and get Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions based on all datasets\n",
        "pred_values_train_tf = model_inference(input_train_inf)\n",
        "pred_values_val_tf = model_inference(input_val_inf)\n",
        "pred_values_test_tf = model_inference(input_test_inf)\n",
        "\n",
        "# Calculate mae for all datasets\n",
        "mae = tf.keras.metrics.mean_absolute_error\n",
        "train_mae = mae(np.squeeze(pred_values_train_tf), np.squeeze(true_output_train_inf)).numpy()*VALUES_RANGE\n",
        "val_mae = mae(np.squeeze(pred_values_val_tf), np.squeeze(true_output_val_inf)).numpy()*VALUES_RANGE\n",
        "test_mae = mae(np.squeeze(pred_values_test_tf), np.squeeze(true_output_test_inf)).numpy()*VALUES_RANGE\n",
        "\n",
        "# Print maes\n",
        "print(f'TRAIN MAE: {round((train_mae),3)} ~ {round(100*train_mae/VALUES_RANGE,3)} %')\n",
        "print(f'VALIDATION MAE: {round((val_mae),3)} ~ {round(100*val_mae/VALUES_RANGE,3)} %')\n",
        "print(f'TEST MAE: {round((test_mae),3)} ~ {round(100*test_mae/VALUES_RANGE,3)} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 8.3 Plot predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph the predictions against the actual values\n",
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "plt.title('Predictions vs Actual Values')\n",
        "\n",
        "plt.plot(time_train, true_values_train, 'b', label='Train set')\n",
        "plt.plot(time_train[WINDOW_SIZE:len(pred_values_train_tf)+WINDOW_SIZE], pred_values_train_tf, 'k', label='Predictions')\n",
        "plt.plot(time_validate, true_values_validate, 'r', label='Validation Set')\n",
        "plt.plot(time_validate[WINDOW_SIZE:len(pred_values_val_tf)+WINDOW_SIZE], pred_values_val_tf, 'k')\n",
        "plt.plot(time_test, true_values_test, 'g', label='Test Set')\n",
        "plt.plot(time_test[WINDOW_SIZE:len(pred_values_test_tf)+WINDOW_SIZE], pred_values_test_tf, 'k')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate and Train quantization aware Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Define quantization config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RNN Quant Config\n",
        "class Quant_Config(tfmot.quantization.keras.QuantizeConfig):\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      return [(layer.cell.recurrent_kernel, tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      layer.cell.recurrent_kernel = quantize_weights[0]\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      layer.activations = quantize_activations[0]\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "    def get_config(self):\n",
        "      return {}\n",
        "\n",
        "# LSTM layers should be quantized\n",
        "def apply_quantization_to_LSTM(layer):\n",
        "  if isinstance(layer, tf.keras.layers.LSTM):\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer, Quant_Config())\n",
        "  return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Train quantization aware model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# anotate the lstm layers\n",
        "quant_aware_model = tf.keras.models.clone_model(model,clone_function=apply_quantization_to_LSTM)\n",
        "\n",
        "# Make model quant aware\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_annotate_model(quant_aware_model)\n",
        "with tfmot.quantization.keras.quantize_scope({'Quant_Config': Quant_Config}):\n",
        "    quant_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_model)\n",
        "\n",
        "# quantization requires a recompile\n",
        "Compile_Model(quant_aware_model)\n",
        "\n",
        "# Train the quantization aware model\n",
        "history_quant = quant_aware_model.fit( input_train, true_output_train,\n",
        "                        validation_data=(input_val, true_output_val),\n",
        "                        epochs=EPOCHS, \n",
        "                        batch_size=BATCH_SIZE_TRAIN, \n",
        "                        shuffle=True,\n",
        "                        callbacks=[early_stop_and_restore_weights],\n",
        "                        verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Fix Model Input Size for Inference and show summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# anotate the lstm layers of the inference model\n",
        "quant_aware_model_inference = tf.keras.models.clone_model(model_inference, clone_function=apply_quantization_to_LSTM)\n",
        "\n",
        "# Make the inference model quant aware\n",
        "quant_aware_model_inference = tfmot.quantization.keras.quantize_annotate_model(quant_aware_model_inference)\n",
        "with tfmot.quantization.keras.quantize_scope({'Quant_Config': Quant_Config}):\n",
        "    quant_aware_model_inference = tfmot.quantization.keras.quantize_apply(quant_aware_model_inference)\n",
        "\n",
        "# Set the weights trained with quantization aware model\n",
        "quant_aware_model_inference.set_weights(quant_aware_model.get_weights())\n",
        "\n",
        "# Save model as Keras and tf model\n",
        "quant_aware_model_inference.save(MODEL_KERAS_QUANT, save_format=\"h5\")\n",
        "quant_aware_model_inference.save(MODEL_TF_QUANT, save_format=\"tf\")\n",
        "\n",
        "# show Model summary\n",
        "quant_aware_model_inference.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Plot Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.1 Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Data\n",
        "train_loss_quant = history_quant.history['loss']\n",
        "val_loss_quant = history_quant.history['val_loss']\n",
        "\n",
        "# plot\n",
        "SKIP = 1\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.plot(np.arange(1, len(val_loss_quant)+1, len(val_loss_quant)/len(train_loss_quant))[SKIP:], train_loss_quant[SKIP:], 'k', label='Training loss')\n",
        "plt.plot(np.arange(1, len(val_loss_quant)+1)[SKIP:], val_loss_quant[SKIP:], 'k--', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2 Predict Output for all Datasets and get Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions based on all datasets\n",
        "pred_values_train_quant = quant_aware_model_inference(input_train_inf)\n",
        "pred_values_val_quant = quant_aware_model_inference(input_val_inf)\n",
        "pred_values_test_quant = quant_aware_model_inference(input_test_inf)\n",
        "\n",
        "# Calculate mae for all datasets\n",
        "train_mae_quant = mae(np.squeeze(pred_values_train_quant), np.squeeze(true_output_train_inf)).numpy()*VALUES_RANGE\n",
        "val_mae_quant = mae(np.squeeze(pred_values_val_quant), np.squeeze(true_output_val_inf)).numpy()*VALUES_RANGE\n",
        "test_mae_quant = mae(np.squeeze(pred_values_test_quant), np.squeeze(true_output_test_inf)).numpy()*VALUES_RANGE\n",
        "\n",
        "# Print maes\n",
        "print(f'TRAIN MAE: {round((train_mae_quant),3)} ~ {round(100*train_mae_quant/VALUES_RANGE,3)} %')\n",
        "print(f'VALIDATION MAE: {round((val_mae_quant),3)} ~ {round(100*val_mae_quant/VALUES_RANGE,3)} %')\n",
        "print(f'TEST MAE: {round((test_mae_quant),3)} ~ {round(100*test_mae_quant/VALUES_RANGE,3)} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3 Plot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph the predictions against the actual values\n",
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "plt.title('Predictions vs Actual Values')\n",
        "\n",
        "plt.plot(time_train, true_values_train, 'b', label='Train set')\n",
        "plt.plot(time_train[WINDOW_SIZE:len(pred_values_train_quant)+WINDOW_SIZE], pred_values_train_quant, 'k', label='Predictions Quantized Model')\n",
        "plt.plot(time_validate, true_values_validate, 'r', label='Validation Set')\n",
        "plt.plot(time_validate[WINDOW_SIZE:len(pred_values_val_quant)+WINDOW_SIZE], pred_values_val_quant, 'k')\n",
        "plt.plot(time_test, true_values_test, 'g', label='Test Set')\n",
        "plt.plot(time_test[WINDOW_SIZE:len(pred_values_test_quant)+WINDOW_SIZE], pred_values_test_quant, 'k')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h7IcvuOOS4J"
      },
      "source": [
        "\n",
        "## Generate a TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHe-Wv47rhm8"
      },
      "source": [
        "### 1. Generate TF Lite Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1muAoUm8lSXL"
      },
      "outputs": [],
      "source": [
        "# Init converter\n",
        "converter_quant = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model_inference)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_inference)\n",
        "\n",
        "# Set Optimizations\n",
        "converter_quant.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert and save the tf lite model\n",
        "model_tflite_quant = converter_quant.convert()\n",
        "model_tflite = converter.convert()\n",
        "open(MODEL_TFLITE_QUANT, \"wb\").write(model_tflite_quant)\n",
        "open(MODEL_TFLITE, \"wb\").write(model_tflite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_vE-ZDkHVxe"
      },
      "source": [
        "### 2. Compare Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Define Functin for Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NKtmxEhko1S1"
      },
      "outputs": [],
      "source": [
        "def predict_tflite(tflite_model, input):\n",
        "  #  Load Test Data\n",
        "  input_data = input.astype('float32')\n",
        "  input_shape = (1, input_data.shape[1], input_data.shape[2])\n",
        "  \n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.resize_tensor_input(0, input_shape);\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_details = interpreter.get_output_details()[0][\"index\"]\n",
        "  \n",
        "  # Invoke the interpreter\n",
        "  predictions = []\n",
        "  for input_datapoint in input_data:\n",
        "    interpreter.set_tensor(input_details, [input_datapoint])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.tensor(output_details)\n",
        "    predictions.append(output()[0])\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZLY0D4gl6U"
      },
      "source": [
        "**2. Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J7IKlXiYVPz"
      },
      "outputs": [],
      "source": [
        "# Calculate predictions\n",
        "pred_values_test_tflite_quant = [predict_tflite(model_tflite_quant, np.expand_dims(input_sequence, axis=0)) for input_sequence in input_test_inf]\n",
        "pred_values_test_tflite_quant = np.squeeze(pred_values_test_tflite_quant)\n",
        "pred_values_test_tflite = [predict_tflite(model_tflite, np.expand_dims(input_sequence, axis=0)) for input_sequence in input_test_inf]\n",
        "pred_values_test_tflite = np.squeeze(pred_values_test_tflite)\n",
        "\n",
        "# Compare predictions\n",
        "plt.clf()\n",
        "plt.title('Comparison of various models against actual values')\n",
        "plt.plot(time_test, true_values_test, 'k.', label='Actual values')\n",
        "plt.plot(time_test[WINDOW_SIZE:len(pred_values_test_tf)+WINDOW_SIZE], pred_values_test_tf, 'bo', label='TF predictions')\n",
        "plt.plot(time_test[WINDOW_SIZE:], pred_values_test_tflite_quant, 'r.', label='TFLite Quant predictions')\n",
        "plt.plot(time_test[WINDOW_SIZE:], pred_values_test_tflite, 'g.', label='TFLite predictions')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vlfJqbiZMU"
      },
      "source": [
        "**2. Loss (MAE/Mean Absolut Error)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpHifyGZRhw8"
      },
      "outputs": [],
      "source": [
        "# Get mae of tflite model on train set\n",
        "mae_tflite_quant = mae(np.squeeze(true_output_test_inf), pred_values_test_tflite_quant).numpy()*VALUES_RANGE\n",
        "mae_tflite = mae(np.squeeze(true_output_test_inf), pred_values_test_tflite).numpy()*VALUES_RANGE\n",
        "\n",
        "# Compare MAE\n",
        "print('MAE BASE:', round((test_mae),3),'~',round(100*test_mae/VALUES_RANGE,3),'%')\n",
        "print(f'MAE QUANT AWARE: {round((test_mae_quant),3)} ~ {round(100*test_mae_quant/VALUES_RANGE,3)} %')\n",
        "print('MAE TF LITE:', round((mae_tflite),3),'~',round(100*mae_tflite/VALUES_RANGE,3),'%')\n",
        "print('MAE TF LITE QUANT:', round((mae_tflite_quant),3),'~',round(100*mae_tflite_quant/VALUES_RANGE,3),'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPSFmDL7pv2L"
      },
      "source": [
        "## Generate a TensorFlow Lite for Microcontrollers Model\n",
        "Convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGf60oJY9KVh"
      },
      "outputs": [],
      "source": [
        "# Convert Model to C\n",
        "source_text_quant, _ = convert_bytes_to_c_source(model_tflite_quant, \"model\")\n",
        "source_text, _ = convert_bytes_to_c_source(model_tflite, \"model\")\n",
        "\n",
        "# Save C File as Header\n",
        "with  open(MODEL_TFLITE_HEADER_QUANT,  'w')  as  file:\n",
        "    file.write('#ifndef MODEL_DATA_H\\n#define MODEL_DATA_H\\n\\n'+source_text_quant+'\\n\\n#endif //MODEL_DATA_H')\n",
        "\n",
        "with  open(MODEL_TFLITE_HEADER,  'w')  as  file:\n",
        "    file.write('#ifndef MODEL_DATA_H\\n#define MODEL_DATA_H\\n\\n'+source_text+'\\n\\n#endif //MODEL_DATA_H')\n",
        "\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Time Series Model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "dac494b5a646e4864a42364743232279aabdb40846e1ef73ed5e92c4b5911599"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
